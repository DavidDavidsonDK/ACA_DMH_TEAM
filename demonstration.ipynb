{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from random import randrange\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "YOU MUST!!!\n",
    "Read all the lines of the code provided to you and understand what it does!\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "class DecisionNode(object):\n",
    "    \"\"\"\n",
    "    README\n",
    "    DecisionNode is a building block for Decision Trees.\n",
    "    DecisionNode is a python class representing a  node in our decision tree\n",
    "    node = DecisionNode()  is a simple usecase for the class\n",
    "    you can also initialize the class like this:\n",
    "    node = DecisionNode(column = 3, value = \"Car\")\n",
    "    In python, when you initialize a class like this, its __init__ method is called \n",
    "    with the given arguments. __init__() creates a new object of the class type, and initializes its \n",
    "    instance attributes/variables.\n",
    "    In python the first argument of any method in a class is 'self'\n",
    "    Self points to the object which it is called from and corresponds to 'this' from Java\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 column=None,\n",
    "                 value=None,\n",
    "                 false_branch=None,\n",
    "                 true_branch=None,\n",
    "                 current_results=None,\n",
    "                 is_leaf=False,\n",
    "                 results=None):\n",
    "        self.column = column\n",
    "        self.value = value\n",
    "        self.false_branch = false_branch\n",
    "        self.true_branch = true_branch\n",
    "        self.current_results = current_results\n",
    "        self.is_leaf = is_leaf\n",
    "        self.results = results\n",
    "\n",
    "\n",
    "def dict_of_values(data):\n",
    "    \"\"\"\n",
    "        param data: a 2D Python list representing the data. Last column of data is Y.\n",
    "    return: returns a python dictionary showing how many times each value appears in Y\n",
    "\n",
    "    for example \n",
    "    data = [[1,'yes'],[1,'no'],[1,'yes'],[1,'yes']]\n",
    "    dict_of_values(data)\n",
    "    should return {'yes' : 3, 'no' :1}\n",
    "        \"\"\"\n",
    "    results = defaultdict(int)\n",
    "    for row in data:\n",
    "        r = row[len(row) - 1]\n",
    "        results[r] += 1\n",
    "    return dict(results)\n",
    "\n",
    "\n",
    "def divide_data(data, feature_column, feature_val):\n",
    "    \"\"\"\n",
    "    this function takes the data and divides it in two parts by a line. A line\n",
    "    is defined by the feature we are considering (feature_column) and the target \n",
    "    value. The function returns a tuple (data1, data2) which are the desired parts of the data.\n",
    "    For int or float types of the value, data1 have all the data with values >= feature_val\n",
    "    in the corresponding column and data2 should have rest.\n",
    "    For string types, data1 should have all data with values == feature val and data2 should \n",
    "    have the rest.\n",
    "\n",
    "    param data: a 2D Python list representing the data. Last column of data is Y.\n",
    "    param feature_column: an integer index of the feature/column.\n",
    "    param feature_val: can be int, float, or string\n",
    "    return: a tuple of two 2D python lists\n",
    "        \"\"\"\n",
    "    #empty lists\n",
    "    data1 = []\n",
    "    data2 = []\n",
    "    \n",
    "    if type(feature_val) == int or type(feature_val) == float or type(feature_val) == np.float64:\n",
    "        for datum in data:\n",
    "            if datum[feature_column] >= feature_val:\n",
    "                data1.append(datum)\n",
    "            else:\n",
    "                data2.append(datum)\n",
    "    elif type(feature_val) == str:\n",
    "\n",
    "        for datum in data:\n",
    "            if datum[feature_column] == feature_val:\n",
    "                data1.append(datum)\n",
    "            else:\n",
    "                data2.append(datum)\n",
    "\n",
    "    return data1, data2\n",
    "\n",
    "\n",
    "def gini_impurity(data1, data2):\n",
    "\n",
    "    \"\"\"\n",
    "    Given two 2D lists of compute their gini_impurity index. \n",
    "    Remember that last column of the data lists is the Y\n",
    "    Lets assume y1 is y of data1 and y2 is y of data2.\n",
    "    gini_impurity shows how diverse the values in y1 and y2 are.\n",
    "    gini impurity is given by \n",
    "\n",
    "    N1*sum(p_k1 * (1-p_k1)) + N2*sum(p_k2 * (1-p_k2))\n",
    "\n",
    "    where N1 is number of points in data1\n",
    "    p_k1 is fraction of points that have y value of k in data1\n",
    "    same for N2 and p_k2\n",
    "\n",
    "\n",
    "    param data1: A 2D python list\n",
    "    param data2: A 2D python list\n",
    "    return: a number - gini_impurity \n",
    "    \"\"\"\n",
    "    #get length of data1 and data2\n",
    "    N1 = len(data1)\n",
    "    N2 = len(data2)\n",
    "    \n",
    "    #use dict_of_values to get the answers  for data1 and data2\n",
    "    data1_answ = dict_of_values(data1)\n",
    "    data2_answ = dict_of_values(data2)\n",
    "    \n",
    "    #initialization\n",
    "    data1_gini = 0\n",
    "    data2_gini = 0\n",
    "    \n",
    "    #calculating\n",
    "    if N1 != 0:\n",
    "        data1_gini = sum([(val / N1)*(1.0 - (val / N1)) for val in data1_answ.values()])\n",
    "    if N2 != 0:\n",
    "        data2_gini = sum([(val / N2)*(1.0 - (val / N2)) for val in data2_answ.values()])\n",
    "    \n",
    "    return N1*data1_gini + N2*data2_gini\n",
    "\n",
    "def get_split(dataset):\n",
    "    \"\"\"\n",
    "        Select the best split point for a dataset\n",
    "    \"\"\"\n",
    "    b_column, b_value, b_gini, b_split = None, None, 1e10, None\n",
    "    for index in range(len(dataset[0])-1):\n",
    "        for row in dataset:\n",
    "            split = divide_data(dataset, index, row[index])\n",
    "            gini = gini_impurity(split[0], split[1])\n",
    "            if gini < b_gini:\n",
    "                b_column, b_value, b_gini, b_split = index, row[index], gini, split\n",
    "    return (b_column, b_value, b_gini, b_split)\n",
    "\n",
    "\n",
    "def build_tree(data, current_depth=0, max_depth=1e10):\n",
    "    \"\"\"\n",
    "    build_tree is a recursive function.\n",
    "    What it does in the general case is:\n",
    "    1: find the best feature and value of the feature to divide the data into\n",
    "    two parts\n",
    "    2: divide data into two parts with best feature, say data1 and data2\n",
    "        recursively call build_tree on data1 and data2. this should give as two \n",
    "        trees say t1 and t2. Then the resulting tree should be \n",
    "        DecisionNode(...... true_branch=t1, false_branch=t2) \n",
    "\n",
    "\n",
    "    In case all the points in the data have same Y we should not split any more, \n",
    "    and return that node\n",
    "    For this function we will give you some of the code so its not too hard for you ;)\n",
    "    \n",
    "    param data: param data: A 2D python list\n",
    "    param current_depth: an integer. This is used if we want to limit the numbr of layers in the\n",
    "        tree\n",
    "    param max_depth: an integer - the maximal depth of the representing\n",
    "    return: an object of class DecisionNode\n",
    "\n",
    "    \"\"\"\n",
    "    if len(data) == 0:\n",
    "\n",
    "        return DecisionNode(is_leaf=True)\n",
    "\n",
    "    if current_depth == max_depth:\n",
    "\n",
    "        return DecisionNode(current_results=dict_of_values(data))\n",
    "\n",
    "    if len(dict_of_values(data)) == 1:\n",
    "\n",
    "        return DecisionNode(current_results=dict_of_values(data), is_leaf=True)\n",
    "\n",
    "    #This calculates gini number for the data before dividing \n",
    "    self_gini = gini_impurity(data, [])\n",
    "\n",
    "    \n",
    "    best_column, best_value, best_gini, best_split = get_split(data)\n",
    "    \n",
    "    \n",
    "    #if best_gini is no improvement from self_gini, we stop and return a node.\n",
    "    if abs(self_gini - best_gini) < 1e-10:\n",
    "\n",
    "        return DecisionNode(current_results=dict_of_values(data), is_leaf=True)\n",
    "\n",
    "    else:\n",
    "        \n",
    "        #recursively call build tree, construct the correct return argument and return\n",
    "        t1 = build_tree(best_split[0], current_depth=current_depth+1, max_depth=1e10)\n",
    "        t2 = build_tree(best_split[1], current_depth=current_depth+1, max_depth=1e10)\n",
    "\n",
    "        return DecisionNode(current_results=dict_of_values(data), column=best_column,\n",
    "                            value=best_value, true_branch = t1, false_branch = t2)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def print_tree(tree, indent='^'):\n",
    "    # Is this a leaf node?\n",
    "    if tree.is_leaf:\n",
    "        print(str(tree.current_results))\n",
    "    else:\n",
    "        # Print the criteria\n",
    "        #         print (indent+'Current Results: ' + str(tree.current_results))\n",
    "        print('Column ' + str(tree.column) + ' : ' + str(tree.value) + '? ')\n",
    "\n",
    "        # Print the branches\n",
    "        print(indent + 'True->', end=\"\")\n",
    "        print_tree(tree.true_branch, indent + '  ')\n",
    "        print(indent + 'False->', end=\"\")\n",
    "        print_tree(tree.false_branch, indent + '  ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DecisionTree(object):\n",
    "    \"\"\"\n",
    "    DecisionTree class, that represents one Decision Tree\n",
    "\n",
    "    :param max_tree_depth: maximum depth for this tree.\n",
    "    \"\"\"\n",
    "    def __init__(self, max_tree_depth):\n",
    "        self.max_depth = max_tree_depth\n",
    "    \n",
    "    \n",
    "        \n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        :param X: 2 dimensional python list or numpy 2 dimensional array\n",
    "        :param Y: 1 dimensional python list or numpy 1 dimensional array\n",
    "        \"\"\"\n",
    "\n",
    "        data = np.column_stack((X, Y))\n",
    "      \n",
    "        self.tree = build_tree(data, max_depth=self.max_depth)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        :param X: 2 dimensional python list or numpy 2 dimensional array\n",
    "        :return: Y - 1 dimension python list with labels\n",
    "        \"\"\"\n",
    "        Y = []\n",
    "        tree = self.tree\n",
    "        \n",
    "        for i in range(len(X)):\n",
    "            row = X[i]\n",
    "            tree = self.tree\n",
    "\n",
    "            while tree.is_leaf == False:\n",
    "                \n",
    "                if row[tree.column] >= tree.value:\n",
    "                    tree = tree.true_branch\n",
    "                else:\n",
    "                    tree = tree.false_branch\n",
    "\n",
    "                if tree.is_leaf:\n",
    "                    dict = tree.current_results\n",
    "                    keys = list(dict.keys())\n",
    "                    Y.append(int(keys[0]))\n",
    "            \n",
    "        return Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RandomForest(object):\n",
    "    \"\"\"\n",
    "    RandomForest a class, that represents Random Forests.\n",
    "\n",
    "    :param num_trees: Number of trees in the random forest\n",
    "    :param max_tree_depth: maximum depth for each of the trees in the forest.\n",
    "    :param ratio_per_tree: ratio of points to use to train each of\n",
    "        the trees.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_trees, max_tree_depth, ratio_per_tree=0.5):\n",
    "        self.num_trees = num_trees\n",
    "        self.max_tree_depth = max_tree_depth\n",
    "        self.ratio_per_tree = ratio_per_tree\n",
    "        self.trees = None\n",
    "    \n",
    "    \n",
    " \n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        :param X: 2 dimensional python list or numpy 2 dimensional array\n",
    "        :param Y: 1 dimensional python list or numpy 1 dimensional array\n",
    "        \"\"\"\n",
    "        #empty forest\n",
    "        self.trees = []\n",
    "        \n",
    "        for i in range(self.num_trees):\n",
    "            samplex, sampley = subsample(X, Y, self.ratio_per_tree)\n",
    "            tree = DecisionTree(self.max_tree_depth)\n",
    "            tree.fit(samplex, sampley)\n",
    "            self.trees.append(tree)\n",
    "    \n",
    "\n",
    "\n",
    "          \n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        :param X: 2 dimensional python list or numpy 2 dimensional array\n",
    "        :return: (Y, conf), tuple with Y being 1 dimension python\n",
    "        list with labels, and conf being 1 dimensional list with\n",
    "        confidences for each of the labels.\n",
    "        \"\"\"\n",
    "        Y = [bagging_predict(self.trees, row) for row in X]\n",
    "        \n",
    "        return (Y)\n",
    "\n",
    "def subsample(dataset, responses, ratio):\n",
    "    \"\"\"\n",
    "    Create a random subsample from the dataset with replacement\n",
    "    \n",
    "    \"\"\"\n",
    "    samplex = list()\n",
    "    sampley = list()\n",
    "    \n",
    "    n_sample = round(len(dataset) * ratio)\n",
    "    while len(samplex) < n_sample:\n",
    "        index = randrange(len(dataset))\n",
    "        samplex.append(dataset[index])\n",
    "        sampley.append(responses[index])\n",
    "    return samplex, sampley \n",
    "\n",
    "\n",
    "def bagging_predict(trees, row):\n",
    "    '''\n",
    "    Param: trees -- list of trees(forest)\n",
    "    Param: row -- a row from X\n",
    "    Process: calculate prediction using confidence coeficent\n",
    "    '''\n",
    "    predictions = [tree.predict([row]) for tree in trees]\n",
    "    max1 = 0\n",
    "    for item in predictions:\n",
    "        if item == [1]:\n",
    "            max1 += 1\n",
    "    \n",
    "    if max1 >= len(predictions) - max1:\n",
    "        return  1 \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    This module compute logistic regression model\n",
    "    It uses stohastic gradient descent(SGD)\n",
    "'''\n",
    "\n",
    "def column_means(dataset):\n",
    "    \"\"\"\n",
    "    Param: dataset matirx of our futures\n",
    "    The fisrt element of each of row in dataset is a 1\n",
    "    Process: calculate column means\n",
    "    \"\"\"\n",
    "    means = [0 for i in range(len(dataset[0]))]\n",
    "    for i in range(1, len(dataset[0])):\n",
    "        col_values = [row[i] for row in dataset]\n",
    "        means[i] = sum(col_values) / float(len(dataset))\n",
    "    return means\n",
    "\n",
    "\n",
    "def column_stdevs(dataset, means):\n",
    "    \"\"\"\n",
    "    Param: dataset matirx of our futures\n",
    "    Param: means is a vector of mean values for each column\n",
    "    Process: calculate column standard deviations\n",
    "    \"\"\"\n",
    "    stdevs = [0 for i in range(len(dataset[0]))]\n",
    "    for i in range(1, len(dataset[0])):\n",
    "        variance = [(row[i]-means[i])**2 for row in dataset]\n",
    "        stdevs[i] = sum(variance)\n",
    "    stdevs = [(x/(float(len(dataset)-1)))**0.5 for x in stdevs]\n",
    "    return stdevs\n",
    "\n",
    "\n",
    "def standardize_dataset(dataset, means, stdevs):\n",
    "    \"\"\"\n",
    "    Param: dataset matirx of our futures\n",
    "    Param: means is a vector of mean values for each column\n",
    "    Param: stdevs os avecor of std for each column\n",
    "    Process:standardize dataset\n",
    "    \"\"\"\n",
    "    for row in dataset:\n",
    "        for i in range(1, len(row)):\n",
    "            row[i] = (row[i] - means[i]) / stdevs[i]\n",
    "def norm(a):\n",
    "    \"\"\"\n",
    "    Param: a is a vector\n",
    "    Process: calculate norm of 'a'\n",
    "    \"\"\"\n",
    "    a = np.array(a)\n",
    "    return (np.sum(a**2))**0.5\n",
    "\n",
    "\n",
    "def rescaleBeta(beta, means, std):\n",
    "    \"\"\"\n",
    "    Param: beta is a vector of our hypothesys\n",
    "    Param: means is a vector of mean values for each column\n",
    "    Param: stdevs os avecor of std for each column\n",
    "    Process: rescale beta\n",
    "    \"\"\"\n",
    "    beta[0] = beta[0] - sum([(means[i]*beta[i])/float(std[i]) for i in range(1, len(beta))])\n",
    "    for i in range(1, beta.shape[0]):\n",
    "        beta[i] = beta[i]/float(std[i])\n",
    "\n",
    "def sigmoid(s):\n",
    "    \"\"\"\n",
    "    Param: s is a number i.e int or float\n",
    "    Process: calculate sigmoid function in this point('s')\n",
    "    \"\"\"\n",
    "    return 1.0 / (1 + np.exp(-s))\n",
    "\n",
    "def normalized_gradient(X, Y, beta, lyabdaVector):\n",
    "    \"\"\"\n",
    "    :param X: data matrix (2 dimensional np.array)\n",
    "    :param Y: response variables (1 dimensional np.array)\n",
    "    :param beta: value of beta (1 dimensional np.array)\n",
    "    :param l: regularization parameter lambda\n",
    "    :return: normalized gradient, i.e. gradient normalized according to data\n",
    "    \"\"\"\n",
    "    N = X.shape[0]\n",
    "    gradient = np.zeros(X.shape[1])\n",
    "    gradient = gradient.astype(float)\n",
    "    for i in range(N):\n",
    "        gradient += (-1)*Y[i]*X[i]*(1-sigmoid(Y[i]*X[i].dot(beta)))\n",
    "    gradient += 2*lyabdaVector.dot(beta)\n",
    "\n",
    "    return gradient/float(N)\n",
    "def gradient_descent(X, Y, epsilon=1e-6, l=1, step_size=1e-4, max_steps=1000):\n",
    "    \"\"\"\n",
    "    Implement gradient descent using full value of the gradient.\n",
    "    :param X: data matrix (2 dimensional np.array)\n",
    "    :param Y: response variables (1 dimensional np.array)\n",
    "    :param l: regularization parameter lambda\n",
    "    :param epsilon: approximation strength\n",
    "    :param max_steps: maximum number of iterations before algorithm will\n",
    "        terminate.\n",
    "    :return: value of beta (1 dimensional np.array)\n",
    "    \"\"\"\n",
    "    X = X.astype(float)\n",
    "    means = column_means(X)\n",
    "    std = column_stdevs(X, means)\n",
    "    standardize_dataset(X, means, std)\n",
    "\n",
    "    lyabdaVector = [0]\n",
    "    for i in range(1, len(std)):\n",
    "        lyabdaVector.append((l)/((std[i]))**2)\n",
    "    lyabdaVector = np.array(lyabdaVector)\n",
    "    beta = np.random.random(X.shape[1])\n",
    "    n = X.shape[0]\n",
    "    arange = np.arange(n)\n",
    "    np.random.shuffle(arange)\n",
    "\n",
    "    for s in range(max_steps):\n",
    "        # for each training sample, compute the gradient\n",
    "        index = arange[(s)%n]\n",
    "\n",
    "        gradient = normalized_gradient(X[index:index+1], Y[index:index+1], beta, lyabdaVector)      \n",
    "        # update the beta_temp\n",
    "        prevBeta = beta\n",
    "        beta = beta - step_size * gradient\n",
    "        dif_beta = beta - prevBeta\n",
    "        step_size = step_size - 0.0000000000000000000001\n",
    "        \n",
    "        \n",
    "        if norm(dif_beta)/norm(beta) < epsilon:\n",
    "            print('Converged, iterations:simple gradient ', s, '!!!')\n",
    "            break\n",
    "\n",
    "    rescaleBeta(beta, means, std)\n",
    "    return beta\n",
    "\n",
    "def loss(X, Y, beta):\n",
    "    \"\"\"\n",
    "        Compute loss function\n",
    "    \"\"\"\n",
    "    return  sum([np.log(1 + np.exp(-Y[i]*X[i].dot(beta))) for i in range(X.shape[0])])\n",
    "\n",
    "def logistic_predict(Xtrain, ytrain, Xtest, ytest):\n",
    "    \"\"\"\n",
    "    Param: Xtrain for train SGD model\n",
    "    Param: ytrain response vector for Xtrain\n",
    "    Param: Xtest for train SGD model\n",
    "    Param: ytrain response vector for Xtrain\n",
    "    \"\"\"\n",
    "    \n",
    "    Xtrain = np.array(Xtrain)\n",
    "    ytrain = np.array(ytrain)\n",
    "    Xtest = np.array(Xtest)\n",
    "    ytest = np.array(ytest)\n",
    "    \n",
    "    one_s1 = np.ones(len(Xtrain))\n",
    "    one_s2 = np.ones(len(Xtest))\n",
    "\n",
    "    \n",
    "    for row1, row2 in zip(Xtrain, Xtest):\n",
    "        row1 = np.array(row1)\n",
    "        row2 = np.array(row2)\n",
    "\n",
    "    \n",
    "    Xtrain = np.column_stack((one_s1, Xtrain))\n",
    "    Xtest = np.column_stack((one_s2, Xtest))\n",
    "\n",
    "   \n",
    "\n",
    "    # normalize ytrain and ytest -->[-1,1]\n",
    "    for i in range(len(ytrain)):\n",
    "        if ytrain[i] == 0:\n",
    "            ytrain[i] = -1\n",
    "    \n",
    "    for i in range(len(ytest)):\n",
    "        if ytest[i] == 0:\n",
    "            ytest[i] = -1\n",
    "\n",
    "    beta = gradient_descent(Xtrain, ytrain, epsilon=1e-6, l=1, step_size=1e-2, max_steps=2500)\n",
    "    responses = Xtest.dot(beta)\n",
    "    Y = []\n",
    "    for resp in responses:\n",
    "        if resp > 0:\n",
    "            Y.append(1)\n",
    "        else:\n",
    "            Y.append(0)\n",
    "\n",
    "    return Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part of data (data[0:10]) = \n",
      " [[  1.  59.  52.  70.  67.  73.  66.  72.  61.  58.  52.  72.  71.  70.\n",
      "   77.  66.  65.  67.  55.  61.  57.  68.  66.  72.  74.  63.  64.  56.\n",
      "   54.  67.  54.  76.  74.  65.  67.  66.  56.  62.  56.  72.  62.  74.\n",
      "   74.  64.  67.]\n",
      " [  1.  72.  62.  69.  67.  78.  82.  74.  65.  69.  63.  70.  70.  72.\n",
      "   74.  70.  71.  72.  75.  66.  65.  73.  78.  74.  79.  74.  69.  69.\n",
      "   70.  71.  69.  72.  70.  62.  65.  65.  71.  63.  60.  69.  73.  67.\n",
      "   71.  56.  58.]\n",
      " [  1.  71.  62.  70.  64.  67.  64.  79.  65.  70.  69.  72.  71.  68.\n",
      "   65.  61.  61.  73.  71.  75.  74.  80.  74.  54.  47.  53.  37.  77.\n",
      "   68.  72.  59.  72.  68.  60.  60.  73.  70.  66.  65.  64.  55.  61.\n",
      "   41.  51.  46.]\n",
      " [  1.  69.  71.  70.  78.  61.  63.  67.  65.  59.  59.  66.  69.  71.\n",
      "   75.  65.  58.  60.  55.  62.  59.  67.  66.  74.  74.  64.  60.  57.\n",
      "   54.  70.  73.  69.  76.  62.  64.  61.  61.  66.  65.  72.  73.  68.\n",
      "   68.  59.  63.]\n",
      " [  1.  70.  66.  61.  66.  61.  58.  69.  69.  72.  68.  62.  71.  71.\n",
      "   71.  63.  59.  74.  75.  70.  69.  83.  77.  73.  70.  41.  37.  39.\n",
      "   40.  58.  46.  75.  73.  65.  66.  67.  69.  70.  66.  70.  64.  60.\n",
      "   55.  49.  41.]\n",
      " [  1.  57.  69.  68.  75.  69.  74.  73.  71.  57.  61.  72.  74.  73.\n",
      "   69.  61.  58.  60.  55.  71.  62.  79.  70.  77.  71.  65.  63.  69.\n",
      "   55.  61.  68.  75.  74.  63.  64.  63.  58.  69.  67.  79.  77.  72.\n",
      "   70.  61.  65.]\n",
      " [  1.  69.  66.  62.  75.  67.  71.  72.  76.  69.  70.  66.  69.  71.\n",
      "   80.  66.  64.  71.  77.  65.  61.  72.  67.  71.  69.  65.  57.  69.\n",
      "   65.  68.  65.  76.  73.  63.  64.  69.  70.  72.  72.  69.  68.  70.\n",
      "   73.  63.  59.]\n",
      " [  1.  61.  60.  60.  62.  64.  72.  68.  67.  74.  68.  76.  70.  74.\n",
      "   71.  76.  74.  74.  70.  75.  66.  69.  62.  65.  60.  66.  65.  68.\n",
      "   59.  64.  59.  72.  65.  55.  56.  66.  66.  66.  60.  60.  58.  60.\n",
      "   67.  49.  52.]\n",
      " [  1.  65.  62.  67.  68.  65.  67.  71.  71.  64.  56.  73.  72.  68.\n",
      "   69.  56.  57.  67.  62.  74.  66.  80.  76.  80.  78.  53.  47.  48.\n",
      "   36.  68.  65.  74.  73.  60.  60.  67.  63.  74.  63.  77.  79.  68.\n",
      "   70.  59.  56.]\n",
      " [  1.  74.  73.  72.  79.  66.  61.  76.  66.  65.  64.  78.  74.  62.\n",
      "   57.  48.  36.  62.  50.  67.  63.  79.  70.  61.  57.  52.  36.  69.\n",
      "   49.  55.  65.  74.  73.  58.  60.  64.  62.  73.  69.  62.  67.  60.\n",
      "   56.  53.  46.]]\n",
      "Decision Tree Accuracy =  0.746153846154  ( 0.0624926031126 )\n",
      "Random Forest Tree Accuracy =  0.811538461538  ( 0.0653846153846 )\n",
      "Logistic Reg. Accuracy =  0.819230769231  ( 0.0689095110276 )\n"
     ]
    }
   ],
   "source": [
    "def accuracy_score(Y_true, Y_predict):\n",
    "    \"\"\"\n",
    "    Param: Y_true real labels\n",
    "    Param : Y_predict predicted lables\n",
    "    Process: Calculate accuracy_score\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    for i in range(len(Y_true)):\n",
    "        if Y_true[i] == Y_predict[i]:\n",
    "            correct += 1\n",
    "    return correct/(len(Y_true))\n",
    "\n",
    "def evaluate_performance():\n",
    "    '''\n",
    "    Evaluate the performance of decision trees and logistic regression,\n",
    "    average over 1,000 trials of 10-fold cross validation\n",
    "\n",
    "    Return:\n",
    "      a matrix giving the performance that will contain the following entries:\n",
    "      stats[0,0] = mean accuracy of decision tree\n",
    "      stats[0,1] = std deviation of decision tree accuracy\n",
    "      stats[1,0] = mean accuracy of logistic regression\n",
    "      stats[1,1] = std deviation of logistic regression accuracy\n",
    "\n",
    "    ** Note that your implementation must follow this API**\n",
    "    '''\n",
    "\n",
    "    # Load Data\n",
    "    filename = 'SPECTF.dat'\n",
    "    data = np.loadtxt(filename, delimiter=',')\n",
    "    X = data[:, 1:]\n",
    "    y = np.array([data[:, 0]]).T\n",
    "    n = X.shape[0]\n",
    "    \n",
    "    \n",
    "    print('part of data (data[0:10]) = \\n {}'.format(data[:10]))\n",
    "    k_folds = 10\n",
    "    n = int(X.shape[0]/k_folds)* k_folds\n",
    "    \n",
    "    all_accuracies_tree = list()\n",
    "    all_accuracies_randforest = list()\n",
    "    all_accuracies_log = list()\n",
    "\n",
    "\n",
    "    \n",
    "    for trial in range(1):\n",
    "        \n",
    "        idx = np.arange(n)\n",
    "        np.random.seed(trial)\n",
    "        np.random.shuffle(idx)\n",
    "        indexes = np.split(idx, k_folds)\n",
    "        \n",
    "        for i in range(k_folds):\n",
    "            \n",
    "            train_set = list(indexes)\n",
    "\n",
    "            a = train_set[i]\n",
    "            train_set.pop(i)\n",
    "                \n",
    "                \n",
    "            Xtest = X[a]\n",
    "            ytest = y[a]\n",
    "               \n",
    "            Xtrain = []\n",
    "            ytrain = []\n",
    "            for ff in train_set:\n",
    "                for row1, row2 in zip(X[ff], y[ff]):\n",
    "                    Xtrain.append(row1)\n",
    "                    ytrain.append(row2)\n",
    "                \n",
    "            \n",
    "            # train the decision tree\n",
    "            classifier = DecisionTree(100)\n",
    "            classifier.fit(Xtrain, ytrain)\n",
    "            y_pred = classifier.predict(Xtest)\n",
    "            accuracy1 = accuracy_score(ytest, y_pred)\n",
    "            all_accuracies_tree.append(accuracy1)\n",
    "\n",
    "            \n",
    "            #train the random forest\n",
    "            classifier = RandomForest(10, 50, 0.1)\n",
    "            classifier.fit(Xtrain, ytrain)\n",
    "            y_pred = classifier.predict(Xtest)\n",
    "            accuracy2 = accuracy_score(ytest, y_pred)\n",
    "            all_accuracies_randforest.append(accuracy2)\n",
    "\n",
    "\n",
    "            # train by logostic regrresion\n",
    "            y_pred = logistic_predict(Xtrain, ytrain, Xtest, ytest)\n",
    "            accuracy3 = accuracy_score(ytest, y_pred)\n",
    "            all_accuracies_log.append(accuracy3)\n",
    "\n",
    "\n",
    "    # compute the training accuracy of the model\n",
    "    meanDecisionTreeAccuracy = np.mean(all_accuracies_tree)\n",
    "    stddevDecisionTreeAccuracy = np.std(all_accuracies_tree)\n",
    "    \n",
    "    meanRandomForestAccuracy = np.mean(all_accuracies_randforest)\n",
    "    stddevRandomForestAccuracy = np.std(all_accuracies_randforest)\n",
    "    \n",
    "    \n",
    "    \n",
    "    meanLogisticRegressionAccuracy = np.mean(all_accuracies_log)\n",
    "    stddevLogisticRegressionAccuracy = np.std(all_accuracies_log)\n",
    "    \n",
    "\n",
    "    # make certain that the return value matches the API specification\n",
    "    stats = np.zeros((3, 2))\n",
    "    stats[0, 0] = meanDecisionTreeAccuracy\n",
    "    stats[0, 1] = stddevDecisionTreeAccuracy\n",
    "    stats[1, 0] = meanRandomForestAccuracy\n",
    "    stats[1, 1] = stddevRandomForestAccuracy\n",
    "    stats[2, 0] = meanLogisticRegressionAccuracy\n",
    "    stats[2, 1] = stddevLogisticRegressionAccuracy\n",
    "    return stats\n",
    "\n",
    "\n",
    "# Do not modify from HERE...\n",
    "if __name__ == \"__main__\":\n",
    "    stats = evaluate_performance()\n",
    "    print(\"Decision Tree Accuracy = \", stats[0, 0], \" (\", stats[0, 1], \")\")\n",
    "    print(\"Random Forest Tree Accuracy = \", stats[1, 0], \" (\", stats[1, 1], \")\")\n",
    "    print(\"Logistic Reg. Accuracy = \", stats[2, 0], \" (\", stats[2, 1], \")\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ...to HERE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
